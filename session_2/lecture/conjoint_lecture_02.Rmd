---
title: |
    | Intro to Conjoint Experiments
    | Session 2
author: Alberto Stefanelli
#date: "`r Sys.Date()`"
fontsize: 10pt 
bibliography: /Users/serg/Library/Mobile Documents/com\~apple\~CloudDocs/academia/library.bib
link-citations: yes
linkcolor: blue
knit: (function(inputFile, encoding) {rmarkdown::render(inputFile, encoding = encoding, output_dir = "html") })
output: 
  beamer_presentation:
    theme: "CambridgeUS" #" PaloAlto"
    colortheme: "beaver"
    fonttheme: "structurebold"
    fig_width: 7
    fig_height: 6
    slide_level: 2
    toc: false
    includes:
      in_header: columns.tex
header-includes:
  - \usepackage{booktabs}
  - \usepackage{float}
  - \usepackage{pdflscape}
  - \usepackage{dcolumn}
  #- \DeclareUnicodeCharacter{301}{-}
  - \widowpenalties 1 150
  - \newcommand{\CI}{\mathrel{\perp\mspace{-10mu}\perp}}
---

## Session 1: Recap 

1. In broader terms, causality is a connection of phenomena that connects one element (the cause) with another elements (effect/outcome/response)
    - it is a **process**. The 1st element of this process is responsible for 2nd and the 2nd dependent on the 1st.
    - causality is temporally bound. The cause(s) must precede the effect and **all lie in its past.**
2. The "fundamental problem of causal inference"
    - We only ever observe one of the two outcomes.
    - We need to relay on assumptions to estimate the causal effect 
3. **No causation without manipulation**. 
    - Avoid to elevate to the status of 'cause' attributes that cannot be, in principle, manipulated
    
    
## Outline block 2

1. Intro to CJs
    - Advantages of CJs
    - Types of CJs
    - Design 
2. Lab session 
    - Solution Ex 1. 
    - ConjointSDT 
    - Exercise 

## Materials

- Lecture's PDF
- Lab
- Exercise

Where to find the material:

- On my [GitHub/conjoint_class](https://albertostefanelli.github.io/conjoint_class/)

## Before starting

- Make sure to [ConjointSDT](https://github.com/astrezhnev/conjointsdt) is installed and working.
- **If you have questions, shoot : )**

## Assignment 

To earn 4 credits, you additionally need to prepare a paper assignment. For the paper, you can analyze the dataset of choice to answer a research question of choice. 

Assignments should have the following structure:

- Short theoretical background and research question
- Description of the dataset used
- Modelling strategy
- Results
- Conclusions

The paper should not be longer than 10 pages.

The deadline is 2 months after the course after the end of the course. 

## What is a conjoint experiment 

1. **Substantively:** An instrument to understand peoples' choices
2. **Technically:**  A (factorial) survey experiment designed to measure (underlying) preferences
    1. Whether showing one attribute as opposed to another would change the respondent’s choice
    2. E.g., why do voters choose one party, candidate, or policy over another? 
3. It deals with options that simultaneously vary across two or more attributes (**multi-dimensionality**)
4. As such, involves trade-offs (e.g., Male Republican VS Female Democrat)
    1. The possibility that option A (Donald Trump) is better than option B (Hilary Clinton) on attribute Gender (Male) while B is better than A on attribute PID (Democrat). 

## How a CJ looks like?

```{r ,echo=FALSE, out.width="90%",fig.cap="",fig.show='hold',fig.align='centre'}
knitr::include_graphics("images/conjoint_teele.png")

```

1. Q: What the causal process involves?
2. Q: What are we manipulating in this example?

## Limitations of traditional observational studies

1. Respondent’s preferences are measured without putting them in a broader context
    - Answers to survey questions can fluctuate considerably depending on which other questions are asked [@kahneman_judgment_1982].
    - E.g. a voter may evaluate a particular policy position differently depending on the other policies bundled in a given party platform.
2. Causal identification and causal ordering is difficult (or almost impossible)
    - Q: Does ideological proximity cause vote choice or is the other way around? 

## Why CJ are relevant for causal analysis?

They go beyond the simple cause-and-effect relationship between a single binary treatment and an outcome variable. 

1. Evaluate the relative explanatory power of different theories, moving beyond tests of a single hypothesis
    - Analysis of more complex causal questions
        -  E.g., is ideology or party identification that drive vote choice ?
    - Relatively cheap and fast to implement 
2. Display high(er) levels of validity 
    - External validity: CJs effectively approximate real-world outcomes [@hainmueller_validating_2015; @auerbach_how_2018]
    - Internal Validity: CJs enhance realism relative to the direct elicitation of preferences on a single dimension 
    - Measurement quality: respondents are found to be consistent even for quite complex tasks [e.g., @bansak_beyond_2019]
3. Reduce social desirability biases such as vote for female (vs male) political candidates [e.g., @teele_ties_2018] or opposition to social housing in respondent’s neighborhood [e.g., @hankinson_when_2018].
4. Good prediction device that can provide insights into practical problems such as policy design and implementation.

# Different CJ designs  

## Rating (scales) (1)

::: columns

:::: column

1. It usually involves only one profile at the time
2. As such, no trade-off between attributes
3. Similar to vignette studies 
    
::::

:::: column

\centering

```{r ,echo=FALSE, out.width="100%",fig.show='hold',fig.align='centre'}
knitr::include_graphics("images/cj_rating.jpg")

```

::::

:::

## Rating (scales) (2)

1. PRO: easy to execute, implement, and model 
2. CON: no trade-offs as it often occurs in real choice settings 
3. CON: often no clear differentiation between profiles 
4. CON: interpretation might be difficult in certain situation
    - what liking a candidate actually means?
    - does liking a candidate would imply that people would actually vote for that candidate? 


## Ranking (1)

::: columns

:::: column

1. It involves more than one profile at the time
2. As such, trade-offs are easier to see
3. Clear differentiation between most-preferred and least-preferred profiles if low number of choices
    
::::

:::: column

\centering

```{r ,echo=FALSE, out.width="100%",fig.show='hold',fig.align='centre'}
knitr::include_graphics("images/cj_ranking.png")

```

::::

:::

## Ranking (2)

1. PRO: easy to execute, implement, and model 
2. CON: causal interpretation might be difficult due to mid ranks 
    - What is the difference between the 6th and the 5th candidate in a 15 candidates rank conjoint
    - Does the ranking a candidate first would imply that people would actually vote for that candidate? 
3. CON: often increase measurement error since ranking is not an easy task [@boyle_comparison_2001]

## Choice-based (1)

2. “The most widely used flavour of conjoint analysis” [@sawtooth_sawtooth_2008]
1. Revealed Preference (RP): real life choices 
    - Clear validity advantages [@diamond_contingent_1994]
    - E.g., which insurance plan peoples have chosen after having compared several different offers
2. **Stated Preference (SP):** experimental setting where we ask respondents **what would they do** instead of observing what they have done
    - E.g., what type of insurance plan people **would choose** after having compared several different offers
    - Can be used to assess the choice decision in an hypothetical or future scenario (relevant for policy implementation)
    - However, what people say in surveys can diverge from what they would do in actual decisions

## Choice-based (2)

\centering

```{r ,echo=FALSE, out.width="100%",fig.show='hold',fig.align='centre'}
knitr::include_graphics("images/conjoint_teele.png")

```

## Respondent's decision making in choice-based CJs (1)

1. Decision strategy selection within a compensatory framework 
2. Probability of selecting a profile formed in two complementary ways [@jenke_using_2020]
    - Within-attribute comparisons (i.e. attribute by attribute)
    - Within-profile decision-making (i.e. profile by profile)
3. Assumption of utility maximizing, omniscient (and tireless respondent) does not hold [@meisner_eye-tracking_2010]

\centering

```{r ,echo=FALSE, out.width="90%",fig.show='hold',fig.align='centre'}
knitr::include_graphics("images/within_between.png")

```

## Respondent's decision making in choice-based CJs (2)

Example of eye tracking experiment with conjoint data [see @meisner_eye-tracking_2010]

\centering

```{r ,echo=FALSE, out.width="60%",fig.show='hold',fig.align='centre'}
knitr::include_graphics("images/eye_traking.png")

```

## Choice-based (4)

1. PRO: High(er) external validity 
    - Trade-off between 2 or 3 profiles is more natural and close to real-life choice scenarios 
2. PRO: Quite easy modelling approaches 
3. CON: Careful experimental design 
4. CON: Can be difficult to set up 
5. CON: It cognitively more demanding that simpler designs 

## Other designs

1. Point allocation
2. Combination of previous design 
   - E.g., rating with choice based
   -  E.g., ranking conjoint with an adaptive design 
4. Adaptive Conjoint
    - Adaptive choice-sets based on respondent's characteristics, preferences, or attitudes
    - E.g., in the choice of transport mode, the choice set of an individual without driving license and/or car should not include the alternative "car" 
    - More complex adaptive designs that involve algorithms to cluster individuals based on their underlying preferences 
    - Recently used in combination with large language models [@velez_trade-offs_2023]

## Terminology (1)

1. Alternatives
    - The two (or more) profiles that a respondent will have to choose from (e.g., Candidate A VS Candidate B)
2. Attributes 
    - Independent variables, explanatory variables, treatments, features 
    - Characteristics of the different alternatives (e.g, age, gender of the proposed candidates)
3. Levels (of the attributes)
    - The discrete values that an attribute can take(e.g., Age: 20, 30, 40, 50) 
4. Choice-sets: a combination of two or more alternatives (e.g., only progressive candidates VS only conservative candidates)
    - Relevant if we want to analyse a subset of the profiles (e.g., only female candidates) 

## Terminology (2)

```{r ,echo=FALSE, out.width="100%",fig.cap="",fig.show='hold',fig.align='centre'}
knitr::include_graphics("images/conjoint_teele_explanation_attributes.png")

```

Q: What the research is controlling (and manipulating) and what not?

# The challenge of experimental design

## Why a challenge?

- Why is it called design? 
    - Manipulation of the treatment conditions occurs **by design** 
    - That is, the researcher controls the specific treatment conditions
- Design is one the greater barriers in answering your research question(s) 
- It allow us to understand the decision rules used to select a profile instead of another 

## Design characteristics in a CJ

1. **Feature Design**
    - Attributes
    - Levels 
2. **Choice-Set Design**
    - Conjoint profiles
3. **Information/Context Design**
    - Priming, framing .... 
    - E.g. information acceleration processes, elaboration tasks, information sources

## Feature Design: attributes 

1. Identify and define the **attributes**   
    1. Which attributes should be included in order to answer my RQ?
    2. Define a universal but finite list of attributes that are theoretically relevant
        - Usually 10-15 attributes 
    3. Reduce the attributes
        - Less then 10 attributes is considered acceptable [@bansak_beyond_2019]
        - To ease the cognitive burden of the respondent no more than 5-6 attitudes are recommended 
    4. Avoid double-barrel attributes 
        - E.g., "Expertise" of a candidate
        - Q: can you think of another example?

## Feature Design: levels

1. Identify and define the **levels**   
    1. Which levels are suitable for the decision task?
    2. Are the levels ordinal, nominal or continuous?
    3. Select theoretically relevant start- and end-points for each attributes
        - Q: What is a sensible range for the age of a candidate?
    2. Define a finite list of levels that have discriminant power.
        - From 2 to maximum 8 [@bansak_beyond_2019]
        - Q: What age levels should we include for a candidate?
        - Q: Does a year difference make any difference in accuracy? 
    3. If the design is too big and you assume linearity, use only start- and end-point designs 
        - E.g., Instead of ordinal age use "Young" and "Old"
    4. Avoid attributes ambiguity 
        - E.g., "High level of integrity" of a candidate VS "Never indicted for corruption"  

## Attributes Design: treath to validity {.allowframebreaks} 

1. Internal Validity:
    1. **Masking** or inter-attributes correlation [@verlegh_range_2002]
        - Perceived association between an attribute included in the design and an omitted one
        - E.g. Including policy positions but excluding PID 
    2. **Satisficing** [@bansak_beyond_2019]
        - Disregard substantive information due to high cognitive complexity. 
        - E.g., inferring from PID policy positions and attributes 
    3. **Unrealistic trade-off(s)** between attributes
        1. A doctor (Attribute: Profession) without an academic degree (Attribute: Education)
    4. **“Range effect”** [@verlegh_range_2002]
        - Using specific level combinations could lead to implicit associations 
        - E.g., a white Democratic candidate of age 78 from Vermont
2. External Validity: **“Number-of-levels effect”**
    - Too many levels could make the interpretation difficult 
    - Statistical power might be insufficient 

## Choice-Set Design: Profiles, Tasks, Outcome

1. Number of Alternative **Profiles**
    - Single Profiles (1)
    - Paired Profiles (2) 
    - Multi Profiles (up to 4)
2. Number of conjoint choice **tasks** per respondent 
    - From 2 to no more than 20 
    - If more than 8, it is recommended to have breaks between conjoint sets
3. **Outcome** measures 
    - Discrete choice (better for trade-offs)
    - Individual ratings (better for detecting extreme response style)
4. **Randomization**
    - Position of the attributes in our CJ 
    
## Choice-Set Design: Sample size and complexity 

1. Sample size = Number of choices per individual (tasks) X Number of participant in the study (N) X Number of profiles
    - Trade off between statistical power and fatigue 
2. Design Complexity = Number of Attributes X Number of Attributes' Levels X Number of Profiles 
    - It is a trade-off between cognitive burden and experimental conditions
3. Data quality = Design Complexity + Cognitive Burden

## Conjoint Design: Summary

1. Conjoint experiments are experimental design to measure (underlying) preferences
    - Whether a respondents prefer one attribute as opposed to another when choosing a profile 
2. Advantages
    - Analysis of more complex causal questions
    - Good external and internal validity 
3. Different designs 
    - Rating 
    - Ranking 
    - Choice (most used)
4. Design characteristics in a CJ
    - Feature
    - Choice-set
    - Information
5. Garbage in Garbage out
    - Clarity in the attributes/levels
    - Design Complexity
    - Fatigue/Cognitive Burden
    
## References {.allowframebreaks} 

